# Deep Learning Project: Adversarial Attacks

This project investigates how deep neural networks can be fooled using adversarial examples. Using a pretrained ResNet-34, we apply three popular attack techniquesâ€”FGSM, PGD, and Patch Attackâ€”and evaluate their impact on classification accuracy.

## ğŸ“ File: `Adversarial_Attacks_Final.ipynb`

This notebook contains:
- Setup and evaluation of ResNet-34 on clean data.
- Implementation of:
  - **FGSM (Fast Gradient Sign Method)**
  - **PGD (Projected Gradient Descent)**
  - **Patch-Based Attack**
- Visual comparison of original vs adversarial images.
- Transferability analysis of adversarial examples to DenseNet-121.

---

## ğŸ“Š Key Results

| Attack Type     | Accuracy (â†“) | Accuracy Drop | Transferability to DenseNet-121 |
|-----------------|--------------|----------------|-------------------------------|
| Clean Images    | **70.40%**   | â€”              | â€”                             |
| FGSM Attack     | 39.00%       | â†“ 44.60%       | 94.37%                         |
| PGD Attack      | 38.80%       | â†“ 44.89%       | 84.33%                         |
| Patch Attack    | 27.20%       | â†“ 61.36%       | 61.47%                         |

> âœ… **Observation:** The Patch Attack was highly effective, reducing accuracy the most despite modifying only a small image region.

---

## ğŸ› ï¸ Technologies Used

- Python 3.x
- PyTorch
- Torchvision
- Matplotlib
- NumPy

---

## â–¶ï¸ How to Run

### âœ… Option 1: Google Colab (Recommended)
1. Open [Google Colab](https://colab.research.google.com/).
2. Upload `Adversarial_Attacks.ipynb`.
3. Go to **Runtime > Change runtime type**, and select **GPU**.
4. Run all cells sequentially.

**To unzip and load a dataset in Colab:**
```python
from zipfile import ZipFile
zip_path = "/content/your_dataset.zip"
with ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall("/content/")

### âœ… Option 2: Jupyter Notebook (Local)

```bash
# 1. Install dependencies
pip install torch torchvision matplotlib

# 2. Open the notebook
jupyter notebook Adversarial_Attacks_Final.ipynb

# 3. Run each cell sequentially.
# GPU acceleration is recommended for PGD and Patch attacks


## ğŸ¯ Project Goals

- Demonstrate the vulnerability of image classifiers to adversarial attacks.
- Compare different attack strategies.
- Measure how attacks transfer across architectures.

## ğŸ“ Notes

- Models and datasets are handled via `torchvision`

## ğŸ” Attack Parameters
- **FGSM**: Îµ = 0.02
- **PGD**: Îµ = 0.02, Î± = 0.005, iterations = 20
- **Patch Attack**: 32Ã—32 pixel patch, Îµ = 0.5

## ğŸ“Š Dataset
- 500 images subset from ImageNet-1K (100 classes)
- Images preprocessed with ImageNet normalization

